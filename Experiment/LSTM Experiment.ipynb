{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832eb2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bassam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bassam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "!pip install sumy parsel pycountry tensorflow_hub nltk transformers tensorflow\n",
    "!pip install -q nltk pycountry sentence-transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pycountry\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20faac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, subset='train'):\n",
    "    \"\"\"\n",
    "    Load IMDB dataset from given directory.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Path to 'data' folder containing 'train' and 'test'.\n",
    "        subset (str): 'train' or 'test' (default 'train')\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns ['review', 'sentiment']\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    labels = []\n",
    "\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_path = os.path.join(data_dir, subset, label)  # include subset folder\n",
    "        if not os.path.exists(folder_path):\n",
    "            raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    reviews.append(f.read())\n",
    "                    labels.append(1 if label == 'pos' else 0)  # 1 for positive, 0 for negative\n",
    "\n",
    "    return pd.DataFrame({'review': reviews, 'sentiment': labels})\n",
    "\n",
    "# Example usage\n",
    "data_dir = r\"C:\\Users\\bassam\\OneDrive\\Desktop\\task_2\\data\"\n",
    "train_df = load_data(data_dir, subset='train')\n",
    "test_df = load_data(data_dir, subset='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb2ce0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop_duplicates(inplace=True)\n",
    "train_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96b6b85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24904 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     24904 non-null  object\n",
      " 1   sentiment  24904 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 583.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd431d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24801 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     24801 non-null  object\n",
      " 1   sentiment  24801 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 581.3+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5762cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ready=test_df.copy()\n",
    "train_ready=train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68930626",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ready = shuffle(train_ready, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Helper functions for text augmentation\n",
    "# -----------------------------------------------------\n",
    "#wordnet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lem in syn.lemmas():\n",
    "            if lem.name().lower() != word.lower():\n",
    "                synonyms.add(lem.name().replace(\"_\", \" \"))\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "\n",
    "    candidates = [w for w in words if get_synonyms(w)]\n",
    "    if not candidates:\n",
    "        return sentence\n",
    "\n",
    "    for _ in range(n):\n",
    "        word = random.choice(candidates)\n",
    "        synonym = random.choice(get_synonyms(word))\n",
    "        words = [synonym if w == word else w for w in words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def random_deletion(sentence, p=0.1):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 1:\n",
    "        return sentence\n",
    "    new_words = [w for w in words if random.random() > p]\n",
    "    return \" \".join(new_words) if new_words else random.choice(words)\n",
    "\n",
    "def random_swap(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def augment_text(text):\n",
    "    choice = random.choice([\"synonym\", \"swap\", \"delete\"])\n",
    "\n",
    "    if choice == \"synonym\":\n",
    "        return synonym_replacement(text, n=1)\n",
    "    elif choice == \"swap\":\n",
    "        return random_swap(text, n=1)\n",
    "    elif choice == \"delete\":\n",
    "        return random_deletion(text, p=0.15)\n",
    "\n",
    "    return text\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# STEP 2: Oversample minority classes with augmentation\n",
    "# -----------------------------------------------------\n",
    "\n",
    "class_counts = train_ready[\"sentiment\"].value_counts()\n",
    "max_count = class_counts.max()\n",
    "augmented_rows = []\n",
    "for i in range(len(train_ready)):\n",
    "    row = train_ready.iloc[i]\n",
    "    new_desc = augment_text(row[\"review\"])\n",
    "    augmented_rows.append({\"review\": new_desc, \"sentiment\": row[\"sentiment\"]})\n",
    "\n",
    "aug_df = pd.DataFrame(augmented_rows)\n",
    "train_ready = shuffle(pd.concat([train_ready, aug_df], ignore_index=True), random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3771de38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49808 entries, 262 to 15795\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     49808 non-null  object\n",
      " 1   sentiment  49808 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_ready.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3a2c26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24904 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     24904 non-null  object\n",
      " 1   sentiment  24904 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 583.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a013cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all preprocessing steps\n",
    "\n",
    "\n",
    "# Initialize stop_words and lemmatizer globally for efficiency\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # 1. Convert text to lowercase\n",
    "    def to_lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    # 2. Remove punctuation and numbers\n",
    "    def remove_punctuation_numbers(text):\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        text = re.sub(r'\\b(\\d+)(st|nd|rd|th)\\b', '', text)  # Remove Ordered Numbering\n",
    "        text = re.sub(r'\\b(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:t(?:ember)?)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\\b', '', text)\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "\n",
    "    # 3. Tokenize text into words\n",
    "    def tokenize(text):\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "    # 4. Modified remove_stopwords function: now operates on a list of tokens\n",
    "    def remove_stopwords(tokens):\n",
    "        return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Apply lemmatization to reduce words to their base (dictionary) form\n",
    "    def apply_lemmatization(tokens):\n",
    "        return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    text = to_lower(text)\n",
    "    text = remove_punctuation_numbers(text)\n",
    "\n",
    "    # Original country removal logic, now applied to the string before tokenization\n",
    "    for c in pycountry.countries:\n",
    "        # Using re.escape for robustness against special characters in country names\n",
    "        text = re.sub(r'\\b' + re.escape(c.name.lower()) + r'\\b', '', text)\n",
    "\n",
    "    tokens = tokenize(text) # Tokenize the cleaned string\n",
    "    tokens = remove_stopwords(tokens) # Call the modified remove_stopwords with tokens\n",
    "    tokens = apply_lemmatization(tokens) # Apply lemmatization to the tokens\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# 6- Apply Preprocessing\n",
    "train_ready[\"review\"] = train_ready[\"review\"].apply(preprocess_text)\n",
    "test_ready[\"review\"] = test_ready[\"review\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64275617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92d67c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went saw movie last night coaxed friend mine ill admit reluctant see knew ashton kutcher able comedy wrong kutcher played character jake fischer well kevin costner played ben randall professionalism sign good movie toy emotion one exactly entire theater sold overcome laughter first half movie moved tear second half exiting theater saw many woman tear many full grown men well trying desperately let anyone see cry movie great suggest go see judge'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ready['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da521685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/CSV/balanced_preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a922e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_ready[\"review\"].values\n",
    "X_test  = test_ready[\"review\"].values\n",
    "\n",
    "y_train = train_ready[\"sentiment\"].values\n",
    "y_test  = test_ready[\"sentiment\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9ca3045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (49808,)\n",
      "y_train shape: (49808,)\n",
      "X_test shape: (24801,)\n",
      "y_test shape: (24801,)\n",
      "\n",
      "Training labels distribution:\n",
      "1    24944\n",
      "0    24864\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test labels distribution:\n",
      "1    12440\n",
      "0    12361\n",
      "Name: count, dtype: int64\n",
      "Data is correctly aligned!\n"
     ]
    }
   ],
   "source": [
    "#safety Check\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "print(\"\\nTraining labels distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "print(\"\\nTest labels distribution:\")\n",
    "print(pd.Series(y_test).value_counts())\n",
    "\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "print(\"Data is correctly aligned!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f92d4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "max_len = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d8685eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_pad, y_train = smote.fit_resample(\n",
    "    X_train_pad,\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53604105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bassam\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:940: UserWarning: Layer 'conv1d_7' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  else:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1247/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - accuracy: 0.5212 - loss: 0.9281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 47ms/step - accuracy: 0.5213 - loss: 0.9279 - val_accuracy: 0.8225 - val_loss: 0.6066 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - accuracy: 0.8565 - loss: 0.4738"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 45ms/step - accuracy: 0.8565 - loss: 0.4737 - val_accuracy: 0.9045 - val_loss: 0.3810 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001B[1m1247/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - accuracy: 0.9339 - loss: 0.2994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 45ms/step - accuracy: 0.9339 - loss: 0.2994 - val_accuracy: 0.9234 - val_loss: 0.3302 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001B[1m1247/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - accuracy: 0.9588 - loss: 0.2360"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 45ms/step - accuracy: 0.9588 - loss: 0.2360 - val_accuracy: 0.9385 - val_loss: 0.2925 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001B[1m1247/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - accuracy: 0.9725 - loss: 0.2025"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 45ms/step - accuracy: 0.9725 - loss: 0.2025 - val_accuracy: 0.9421 - val_loss: 0.2455 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001B[1m1247/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - accuracy: 0.9797 - loss: 0.1475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 45ms/step - accuracy: 0.9797 - loss: 0.1475 - val_accuracy: 0.9439 - val_loss: 0.2240 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - accuracy: 0.9833 - loss: 0.1320"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 45ms/step - accuracy: 0.9833 - loss: 0.1320 - val_accuracy: 0.9479 - val_loss: 0.2127 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001B[1m1247/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - accuracy: 0.9875 - loss: 0.1175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m63s\u001B[0m 51ms/step - accuracy: 0.9875 - loss: 0.1175 - val_accuracy: 0.9506 - val_loss: 0.1927 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 45ms/step - accuracy: 0.9904 - loss: 0.1022 - val_accuracy: 0.9509 - val_loss: 0.1954 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001B[1m1247/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - accuracy: 0.9916 - loss: 0.0967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1248/1248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 45ms/step - accuracy: 0.9916 - loss: 0.0967 - val_accuracy: 0.9524 - val_loss: 0.1857 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, LSTM, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# ==========================\n",
    "# Model Definition\n",
    "# ==========================\n",
    "model = Sequential([\n",
    "    \n",
    "    Embedding(20000, 128, input_length=max_len, mask_zero=True),\n",
    "\n",
    "    Conv1D(64, 5, activation=\"relu\", kernel_regularizer=l2(0.002)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    LSTM(32, return_sequences=True, kernel_regularizer=l2(0.002)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    GlobalMaxPooling1D(),\n",
    "\n",
    "    \n",
    "    Dense(16, activation=\"relu\", kernel_regularizer=l2(0.002)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\"best_cnn_lstm_model.h5\", save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    epochs=10,           \n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e7370582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m776/776\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 10ms/step - accuracy: 0.8452 - loss: 0.4905\n",
      "Test Accuracy: 0.8325470685958862\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_pad, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8e27a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m776/776\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 10ms/step\n",
      "Confusion Matrix:\n",
      " [[10085  2276]\n",
      " [ 1877 10563]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83     12361\n",
      "           1       0.82      0.85      0.84     12440\n",
      "\n",
      "    accuracy                           0.83     24801\n",
      "   macro avg       0.83      0.83      0.83     24801\n",
      "weighted avg       0.83      0.83      0.83     24801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred_prob = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int64\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "99c549f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.94440687]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [\"This movie was amazing\"]\n",
    "sample_seq = tokenizer.texts_to_sequences(sample)\n",
    "sample_pad = pad_sequences(sample_seq, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "prediction = model.predict(sample_pad)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
