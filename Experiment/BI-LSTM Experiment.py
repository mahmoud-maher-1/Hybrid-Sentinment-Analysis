# -*- coding: utf-8 -*-
"""Untitled73.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OXgcvqUdnbgLT5b-Wzh9aer60msrO1E8
"""

import os
zip_path = "/content/aclImdb_v1.tar.gz"  # غيّري الاسم حسب ملفك
if zip_path.endswith(".zip"):
    !unzip -q "{zip_path}" -d "/content/imdb_data"
elif zip_path.endswith(".tar.gz") or zip_path.endswith(".tgz"):
    !mkdir -p "/content/imdb_data"
    !tar -xzf "{zip_path}" -C "/content/imdb_data"
print("Folders after extract:")
!ls -R /content/imdb_data

DATA_DIR_ROOT = "/content/imdb_data"
DATA_DIR = os.path.join(DATA_DIR_ROOT, "aclImdb")
TRAIN_DIR = os.path.join(DATA_DIR, "train")
TEST_DIR = os.path.join(DATA_DIR, "test")

print(f"New TRAIN_DIR: {TRAIN_DIR}")

import pandas as pd
def load_data(data_path):

    reviews = []
    labels = []

    pos_path = os.path.join(data_path, 'pos')
    for file_name in os.listdir(pos_path):
        with open(os.path.join(pos_path, file_name), 'r', encoding='utf8') as file:
            reviews.append(file.read())
            labels.append(1)

    neg_path = os.path.join(data_path, 'neg')
    for file_name in os.listdir(neg_path):
        with open(os.path.join(neg_path, file_name), 'r', encoding='utf8') as file:
            reviews.append(file.read())
            labels.append(0)

    df = pd.DataFrame({'review': reviews, 'sentiment': labels})
    return df

train_df = load_data(TRAIN_DIR)
test_df = load_data(TEST_DIR)

print(train_df.head())
print(train_df.shape)

import matplotlib.pyplot as plt
import seaborn as sns
sentiment_counts = train_df['sentiment'].value_counts()
print(sentiment_counts)

plt.figure(figsize=(6, 4))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)
plt.title('Distribution of Sentiments in Training Data')
plt.xlabel('Sentiment (0: Negative, 1: Positive)')
plt.ylabel('Number of Reviews')
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.show()

train_df['review_length'] = train_df['review'].apply(lambda x: len(x.split()))

print(train_df['review_length'].describe())

plt.figure(figsize=(10, 6))
sns.boxplot(x='sentiment', y='review_length', data=train_df)
plt.title('Review Length Distribution by Sentiment')
plt.xlabel('Sentiment (0: Negative, 1: Positive)')
plt.ylabel('Review Length (Number of Words)')
plt.xticks([0, 1], ['Negative', 'Positive'])
# تحديد حد y-axis لرؤية أفضل (إهمال القيم الشاذة العالية)
plt.ylim(0, 750)
plt.show()

from wordcloud import WordCloud
positive_reviews = ' '.join(train_df[train_df['sentiment'] == 1]['review'])
negative_reviews = ' '.join(train_df[train_df['sentiment'] == 0]['review'])

wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_pos, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Positive Reviews')
plt.show()

wordcloud_neg = WordCloud(width=800, height=400, background_color='black').generate(negative_reviews)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_neg, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Negative Reviews')
plt.show()

import os
import re
import string
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

DATA_DIR = "/content/imdb_data/aclImdb"

# ------- cleaning function -------
def clean_text(text):
    text = text.lower()
    text = re.sub(r"<.*?>", " ", text)                     # remove HTML tags
    text = re.sub(r"http\S+|www\.\S+", " ", text)          # remove URLs
    text = re.sub(r"\d+", " ", text)                       # remove digits
    text = text.translate(str.maketrans("", "", string.punctuation))  # remove punctuation
    text = re.sub(r"\s+", " ", text).strip()               # remove extra spaces
    return text

# ------- load split (train/test) -------
def load_split_texts(split="train"):
    split_dir = os.path.join(DATA_DIR, split)
    texts, labels = [], []

    for label_name, label in [("pos", 1), ("neg", 0)]:
        folder = os.path.join(split_dir, label_name)
        for fname in os.listdir(folder):
            if not fname.endswith(".txt"):
                continue
            with open(os.path.join(folder, fname), encoding="utf-8") as f:
                raw = f.read()
            texts.append(clean_text(raw))
            labels.append(label)

    return texts, np.array(labels, dtype="int32")

# load raw train & test
train_texts, train_labels = load_split_texts("train")
test_texts,  test_labels  = load_split_texts("test")

print("Train samples:", len(train_texts))
print("Test samples:", len(test_texts))

# ------- split train -> train + val -------
train_texts, val_texts, y_train, y_val = train_test_split(
    train_texts, train_labels,
    test_size=0.2,
    random_state=42,
    stratify=train_labels
)

print("After split ->")
print("Train:", len(train_texts), " Val:", len(val_texts))



from tensorflow.keras.regularizers import l2
import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, SpatialDropout1D,
    Conv1D, MaxPooling1D,
    Bidirectional, LSTM,
    GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate,
    Dense, Dropout
)
from tensorflow.keras.models import Model

# نفترض أن المتغيرات (MAX_LEN, MAX_WORDS) تم تعريفها
VOCAB_SIZE = MAX_WORDS
EMBED_DIM  = 150 # زيادة بعد التضمين
LSTM_UNITS = 128 # زيادة وحدات الـ LSTM لقوة التعلم

def build_cnn_deep_bilstm_ultimate():
    inputs = Input(shape=(MAX_LEN,), name="input_ids")

    # 1. طبقة التضمين التي تتعلم أوزانها من البيانات (Learned Embedding)
    x = Embedding(
        input_dim=VOCAB_SIZE,
        output_dim=EMBED_DIM,
        input_length=MAX_LEN,
        trainable=True,
        name="learned_embedding"
    )(inputs)

    # Spatial Dropout قوي جداً لمكافحة الإفراط في الملاءمة
    x = SpatialDropout1D(0.5)(x)

    # 2. طبقة Conv1D (زيادة L2)
    x = Conv1D(
        filters=128,
        kernel_size=5,
        padding="same",
        activation="relu",
        kernel_regularizer=l2(1e-3) # تعميق L2
    )(x)
    x = MaxPooling1D(pool_size=2)(x)

    # 3. طبقة Deep BiLSTM الأولى (زيادة الوحدات وزيادة Dropout)
    x = Bidirectional(
        LSTM(
            LSTM_UNITS,
            return_sequences=True,
            dropout=0.5, # زيادة Dropout
            recurrent_dropout=0.5 # زيادة Recurrent Dropout
        )
    )(x)

    # 4. طبقة Deep BiLSTM ثانية
    x = Bidirectional(
        LSTM(
            LSTM_UNITS // 2,
            return_sequences=True,
            dropout=0.4,
            recurrent_dropout=0.4
        )
    )(x)

    # 5. الدمج بين Global Max Pooling و Global Average Pooling
    x_avg = GlobalAveragePooling1D()(x)
    x_max = GlobalMaxPooling1D()(x)
    x = Concatenate()([x_avg, x_max])

    # 6. طبقة Dense مع L2 و Dropout قوي
    x = Dense(
        64,
        activation="relu",
        kernel_regularizer=l2(1e-3) # تعميق L2
    )(x)
    x = Dropout(0.5)(x) # زيادة Dropout

    outputs = Dense(1, activation="sigmoid", name="output")(x)

    model = Model(inputs=inputs, outputs=outputs, name="CNN_Deep_BiLSTM_ULTIMATE")
    return model

model = build_cnn_deep_bilstm_ultimate()
model.summary()

# استخدام AdamW (بديل أفضل لـ Adam مع L2)
# يجب تثبيت مكتبة tensorflow_addons أو استخدام implementation مخصصة
# tf.keras.optimizers.Adam(learning_rate=3e-4) هو خيار جيد إذا لم يكن AdamW متاحاً
model.compile(
    loss="binary_crossentropy",
    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), # تخفيض معدل التعلم قليلاً
    metrics=["accuracy"]
)

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

callbacks = [
    EarlyStopping(
        monitor="val_loss",
        patience=3,
        restore_best_weights=True
    ),
    ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=2,
        verbose=1
    )
]

history = model.fit(
    X_train, y_train,
    epochs=15,
    batch_size=128,
    validation_data=(X_val, y_val),
    callbacks=callbacks
)

test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=128)
print(f"Test Loss: {test_loss:.4f}  |  Test Accuracy: {test_acc:.4f}")

